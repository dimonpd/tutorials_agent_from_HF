{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432d503e",
   "metadata": {},
   "source": [
    "# OpenAI Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3a271c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.4/249.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -Uq openai-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7aec0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.71.2 requires grpcio>=1.71.2, but you have grpcio 1.67.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# to use non-openai models, e.g., from Hugging Face\n",
    "!pip install -Uq \"openai-agents[litellm]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc0c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable async in notebook\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "526c20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# set default model for agents\n",
    "os.environ[\"OPENAI_DEFAULT_MODEL\"] = \"gpt-5-mini\"\n",
    "\n",
    "# openai API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20992c80",
   "metadata": {},
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "318e856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun warms glass and stone,  \n",
      "Blue sky folds the city bright—  \n",
      "Sunny streets hum life.\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, function_tool, Runner\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"returns weather info for the specified city.\"\"\"\n",
    "    return f\"The weather in {city} is sunny\"\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Haiku agent\",\n",
    "    instructions=\"Always respond in haiku form\",\n",
    "    model=\"gpt-5-mini\",\n",
    "    tools=[get_weather],\n",
    ")\n",
    "\n",
    "result = await Runner.run(agent, \"What's the weather in New York?\")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1516495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'agents.result.RunResult'>\n",
      "{'_last_agent': Agent(name='Haiku agent',\n",
      "                      handoff_description=None,\n",
      "                      tools=[FunctionTool(name='get_weather',\n",
      "                                          description='returns weather info '\n",
      "                                                      'for the specified city.',\n",
      "                                          params_json_schema={'additionalProperties': False,\n",
      "                                                              'properties': {'city': {'title': 'City',\n",
      "                                                                                      'type': 'string'}},\n",
      "                                                              'required': ['city'],\n",
      "                                                              'title': 'get_weather_args',\n",
      "                                                              'type': 'object'},\n",
      "                                          on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7b64259bf1a0>,\n",
      "                                          strict_json_schema=True,\n",
      "                                          is_enabled=True,\n",
      "                                          tool_input_guardrails=None,\n",
      "                                          tool_output_guardrails=None)],\n",
      "                      mcp_servers=[],\n",
      "                      mcp_config={},\n",
      "                      instructions='Always respond in haiku form',\n",
      "                      prompt=None,\n",
      "                      handoffs=[],\n",
      "                      model='gpt-5-mini',\n",
      "                      model_settings=ModelSettings(temperature=None,\n",
      "                                                   top_p=None,\n",
      "                                                   frequency_penalty=None,\n",
      "                                                   presence_penalty=None,\n",
      "                                                   tool_choice=None,\n",
      "                                                   parallel_tool_calls=None,\n",
      "                                                   truncation=None,\n",
      "                                                   max_tokens=None,\n",
      "                                                   reasoning=Reasoning(effort='low', generate_summary=None, summary=None),\n",
      "                                                   verbosity='low',\n",
      "                                                   metadata=None,\n",
      "                                                   store=None,\n",
      "                                                   prompt_cache_retention=None,\n",
      "                                                   include_usage=None,\n",
      "                                                   response_include=None,\n",
      "                                                   top_logprobs=None,\n",
      "                                                   extra_query=None,\n",
      "                                                   extra_body=None,\n",
      "                                                   extra_headers=None,\n",
      "                                                   extra_args=None),\n",
      "                      input_guardrails=[],\n",
      "                      output_guardrails=[],\n",
      "                      output_type=None,\n",
      "                      hooks=None,\n",
      "                      tool_use_behavior='run_llm_again',\n",
      "                      reset_tool_choice=True),\n",
      " '_last_agent_ref': <weakref at 0x7b6424821a30; to 'Agent' at 0x7b6425a54230>,\n",
      " 'context_wrapper': RunContextWrapper(context=None,\n",
      "                                      usage=Usage(requests=2,\n",
      "                                                  input_tokens=219,\n",
      "                                                  input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
      "                                                  output_tokens=45,\n",
      "                                                  output_tokens_details=OutputTokensDetails(reasoning_tokens=0),\n",
      "                                                  total_tokens=264,\n",
      "                                                  request_usage_entries=[RequestUsage(input_tokens=69,\n",
      "                                                                                      output_tokens=21,\n",
      "                                                                                      total_tokens=90,\n",
      "                                                                                      input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
      "                                                                                      output_tokens_details=OutputTokensDetails(reasoning_tokens=0)),\n",
      "                                                                         RequestUsage(input_tokens=150,\n",
      "                                                                                      output_tokens=24,\n",
      "                                                                                      total_tokens=174,\n",
      "                                                                                      input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
      "                                                                                      output_tokens_details=OutputTokensDetails(reasoning_tokens=0))])),\n",
      " 'final_output': 'Sun warms glass and stone,  \\n'\n",
      "                 'Blue sky folds the city bright—  \\n'\n",
      "                 'Sunny streets hum life.',\n",
      " 'input': \"What's the weather in New York?\",\n",
      " 'input_guardrail_results': [],\n",
      " 'new_items': [ReasoningItem(agent=Agent(name='Haiku agent',\n",
      "                                         handoff_description=None,\n",
      "                                         tools=[FunctionTool(name='get_weather',\n",
      "                                                             description='returns '\n",
      "                                                                         'weather '\n",
      "                                                                         'info '\n",
      "                                                                         'for '\n",
      "                                                                         'the '\n",
      "                                                                         'specified '\n",
      "                                                                         'city.',\n",
      "                                                             params_json_schema={'additionalProperties': False,\n",
      "                                                                                 'properties': {'city': {'title': 'City',\n",
      "                                                                                                         'type': 'string'}},\n",
      "                                                                                 'required': ['city'],\n",
      "                                                                                 'title': 'get_weather_args',\n",
      "                                                                                 'type': 'object'},\n",
      "                                                             on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7b64259bf1a0>,\n",
      "                                                             strict_json_schema=True,\n",
      "                                                             is_enabled=True,\n",
      "                                                             tool_input_guardrails=None,\n",
      "                                                             tool_output_guardrails=None)],\n",
      "                                         mcp_servers=[],\n",
      "                                         mcp_config={},\n",
      "                                         instructions='Always respond in haiku '\n",
      "                                                      'form',\n",
      "                                         prompt=None,\n",
      "                                         handoffs=[],\n",
      "                                         model='gpt-5-mini',\n",
      "                                         model_settings=ModelSettings(temperature=None,\n",
      "                                                                      top_p=None,\n",
      "                                                                      frequency_penalty=None,\n",
      "                                                                      presence_penalty=None,\n",
      "                                                                      tool_choice=None,\n",
      "                                                                      parallel_tool_calls=None,\n",
      "                                                                      truncation=None,\n",
      "                                                                      max_tokens=None,\n",
      "                                                                      reasoning=Reasoning(effort='low', generate_summary=None, summary=None),\n",
      "                                                                      verbosity='low',\n",
      "                                                                      metadata=None,\n",
      "                                                                      store=None,\n",
      "                                                                      prompt_cache_retention=None,\n",
      "                                                                      include_usage=None,\n",
      "                                                                      response_include=None,\n",
      "                                                                      top_logprobs=None,\n",
      "                                                                      extra_query=None,\n",
      "                                                                      extra_body=None,\n",
      "                                                                      extra_headers=None,\n",
      "                                                                      extra_args=None),\n",
      "                                         input_guardrails=[],\n",
      "                                         output_guardrails=[],\n",
      "                                         output_type=None,\n",
      "                                         hooks=None,\n",
      "                                         tool_use_behavior='run_llm_again',\n",
      "                                         reset_tool_choice=True),\n",
      "                             raw_item=ResponseReasoningItem(id='rs_0a967e5f7f7a333c00695d994173988194b0f675818cb33f00', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
      "                             type='reasoning_item'),\n",
      "               ToolCallItem(agent=Agent(name='Haiku agent',\n",
      "                                        handoff_description=None,\n",
      "                                        tools=[FunctionTool(name='get_weather',\n",
      "                                                            description='returns '\n",
      "                                                                        'weather '\n",
      "                                                                        'info '\n",
      "                                                                        'for '\n",
      "                                                                        'the '\n",
      "                                                                        'specified '\n",
      "                                                                        'city.',\n",
      "                                                            params_json_schema={'additionalProperties': False,\n",
      "                                                                                'properties': {'city': {'title': 'City',\n",
      "                                                                                                        'type': 'string'}},\n",
      "                                                                                'required': ['city'],\n",
      "                                                                                'title': 'get_weather_args',\n",
      "                                                                                'type': 'object'},\n",
      "                                                            on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7b64259bf1a0>,\n",
      "                                                            strict_json_schema=True,\n",
      "                                                            is_enabled=True,\n",
      "                                                            tool_input_guardrails=None,\n",
      "                                                            tool_output_guardrails=None)],\n",
      "                                        mcp_servers=[],\n",
      "                                        mcp_config={},\n",
      "                                        instructions='Always respond in haiku '\n",
      "                                                     'form',\n",
      "                                        prompt=None,\n",
      "                                        handoffs=[],\n",
      "                                        model='gpt-5-mini',\n",
      "                                        model_settings=ModelSettings(temperature=None,\n",
      "                                                                     top_p=None,\n",
      "                                                                     frequency_penalty=None,\n",
      "                                                                     presence_penalty=None,\n",
      "                                                                     tool_choice=None,\n",
      "                                                                     parallel_tool_calls=None,\n",
      "                                                                     truncation=None,\n",
      "                                                                     max_tokens=None,\n",
      "                                                                     reasoning=Reasoning(effort='low', generate_summary=None, summary=None),\n",
      "                                                                     verbosity='low',\n",
      "                                                                     metadata=None,\n",
      "                                                                     store=None,\n",
      "                                                                     prompt_cache_retention=None,\n",
      "                                                                     include_usage=None,\n",
      "                                                                     response_include=None,\n",
      "                                                                     top_logprobs=None,\n",
      "                                                                     extra_query=None,\n",
      "                                                                     extra_body=None,\n",
      "                                                                     extra_headers=None,\n",
      "                                                                     extra_args=None),\n",
      "                                        input_guardrails=[],\n",
      "                                        output_guardrails=[],\n",
      "                                        output_type=None,\n",
      "                                        hooks=None,\n",
      "                                        tool_use_behavior='run_llm_again',\n",
      "                                        reset_tool_choice=True),\n",
      "                            raw_item=ResponseFunctionToolCall(arguments='{\"city\":\"New York\"}', call_id='call_bJp6slF1qrOekKI7c1xMuDCS', name='get_weather', type='function_call', id='fc_0a967e5f7f7a333c00695d99421f588194995bdb8970e25267', status='completed'),\n",
      "                            type='tool_call_item'),\n",
      "               ToolCallOutputItem(agent=Agent(name='Haiku agent',\n",
      "                                              handoff_description=None,\n",
      "                                              tools=[FunctionTool(name='get_weather',\n",
      "                                                                  description='returns '\n",
      "                                                                              'weather '\n",
      "                                                                              'info '\n",
      "                                                                              'for '\n",
      "                                                                              'the '\n",
      "                                                                              'specified '\n",
      "                                                                              'city.',\n",
      "                                                                  params_json_schema={'additionalProperties': False,\n",
      "                                                                                      'properties': {'city': {'title': 'City',\n",
      "                                                                                                              'type': 'string'}},\n",
      "                                                                                      'required': ['city'],\n",
      "                                                                                      'title': 'get_weather_args',\n",
      "                                                                                      'type': 'object'},\n",
      "                                                                  on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7b64259bf1a0>,\n",
      "                                                                  strict_json_schema=True,\n",
      "                                                                  is_enabled=True,\n",
      "                                                                  tool_input_guardrails=None,\n",
      "                                                                  tool_output_guardrails=None)],\n",
      "                                              mcp_servers=[],\n",
      "                                              mcp_config={},\n",
      "                                              instructions='Always respond in '\n",
      "                                                           'haiku form',\n",
      "                                              prompt=None,\n",
      "                                              handoffs=[],\n",
      "                                              model='gpt-5-mini',\n",
      "                                              model_settings=ModelSettings(temperature=None,\n",
      "                                                                           top_p=None,\n",
      "                                                                           frequency_penalty=None,\n",
      "                                                                           presence_penalty=None,\n",
      "                                                                           tool_choice=None,\n",
      "                                                                           parallel_tool_calls=None,\n",
      "                                                                           truncation=None,\n",
      "                                                                           max_tokens=None,\n",
      "                                                                           reasoning=Reasoning(effort='low', generate_summary=None, summary=None),\n",
      "                                                                           verbosity='low',\n",
      "                                                                           metadata=None,\n",
      "                                                                           store=None,\n",
      "                                                                           prompt_cache_retention=None,\n",
      "                                                                           include_usage=None,\n",
      "                                                                           response_include=None,\n",
      "                                                                           top_logprobs=None,\n",
      "                                                                           extra_query=None,\n",
      "                                                                           extra_body=None,\n",
      "                                                                           extra_headers=None,\n",
      "                                                                           extra_args=None),\n",
      "                                              input_guardrails=[],\n",
      "                                              output_guardrails=[],\n",
      "                                              output_type=None,\n",
      "                                              hooks=None,\n",
      "                                              tool_use_behavior='run_llm_again',\n",
      "                                              reset_tool_choice=True),\n",
      "                                  raw_item={'call_id': 'call_bJp6slF1qrOekKI7c1xMuDCS',\n",
      "                                            'output': 'The weather in New York '\n",
      "                                                      'is sunny',\n",
      "                                            'type': 'function_call_output'},\n",
      "                                  output='The weather in New York is sunny',\n",
      "                                  type='tool_call_output_item'),\n",
      "               MessageOutputItem(agent=Agent(name='Haiku agent',\n",
      "                                             handoff_description=None,\n",
      "                                             tools=[FunctionTool(name='get_weather',\n",
      "                                                                 description='returns '\n",
      "                                                                             'weather '\n",
      "                                                                             'info '\n",
      "                                                                             'for '\n",
      "                                                                             'the '\n",
      "                                                                             'specified '\n",
      "                                                                             'city.',\n",
      "                                                                 params_json_schema={'additionalProperties': False,\n",
      "                                                                                     'properties': {'city': {'title': 'City',\n",
      "                                                                                                             'type': 'string'}},\n",
      "                                                                                     'required': ['city'],\n",
      "                                                                                     'title': 'get_weather_args',\n",
      "                                                                                     'type': 'object'},\n",
      "                                                                 on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7b64259bf1a0>,\n",
      "                                                                 strict_json_schema=True,\n",
      "                                                                 is_enabled=True,\n",
      "                                                                 tool_input_guardrails=None,\n",
      "                                                                 tool_output_guardrails=None)],\n",
      "                                             mcp_servers=[],\n",
      "                                             mcp_config={},\n",
      "                                             instructions='Always respond in '\n",
      "                                                          'haiku form',\n",
      "                                             prompt=None,\n",
      "                                             handoffs=[],\n",
      "                                             model='gpt-5-mini',\n",
      "                                             model_settings=ModelSettings(temperature=None,\n",
      "                                                                          top_p=None,\n",
      "                                                                          frequency_penalty=None,\n",
      "                                                                          presence_penalty=None,\n",
      "                                                                          tool_choice=None,\n",
      "                                                                          parallel_tool_calls=None,\n",
      "                                                                          truncation=None,\n",
      "                                                                          max_tokens=None,\n",
      "                                                                          reasoning=Reasoning(effort='low', generate_summary=None, summary=None),\n",
      "                                                                          verbosity='low',\n",
      "                                                                          metadata=None,\n",
      "                                                                          store=None,\n",
      "                                                                          prompt_cache_retention=None,\n",
      "                                                                          include_usage=None,\n",
      "                                                                          response_include=None,\n",
      "                                                                          top_logprobs=None,\n",
      "                                                                          extra_query=None,\n",
      "                                                                          extra_body=None,\n",
      "                                                                          extra_headers=None,\n",
      "                                                                          extra_args=None),\n",
      "                                             input_guardrails=[],\n",
      "                                             output_guardrails=[],\n",
      "                                             output_type=None,\n",
      "                                             hooks=None,\n",
      "                                             tool_use_behavior='run_llm_again',\n",
      "                                             reset_tool_choice=True),\n",
      "                                 raw_item=ResponseOutputMessage(id='msg_0a967e5f7f7a333c00695d9943be14819490d780763286cd4a', content=[ResponseOutputText(annotations=[], text='Sun warms glass and stone,  \\nBlue sky folds the city bright—  \\nSunny streets hum life.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'),\n",
      "                                 type='message_output_item')],\n",
      " 'output_guardrail_results': [],\n",
      " 'raw_responses': [ModelResponse(output=[ResponseReasoningItem(id='rs_0a967e5f7f7a333c00695d994173988194b0f675818cb33f00', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
      "                                         ResponseFunctionToolCall(arguments='{\"city\":\"New York\"}', call_id='call_bJp6slF1qrOekKI7c1xMuDCS', name='get_weather', type='function_call', id='fc_0a967e5f7f7a333c00695d99421f588194995bdb8970e25267', status='completed')],\n",
      "                                 usage=Usage(requests=1,\n",
      "                                             input_tokens=69,\n",
      "                                             input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
      "                                             output_tokens=21,\n",
      "                                             output_tokens_details=OutputTokensDetails(reasoning_tokens=0),\n",
      "                                             total_tokens=90,\n",
      "                                             request_usage_entries=[]),\n",
      "                                 response_id='resp_0a967e5f7f7a333c00695d99411b648194967107f2b6b43984'),\n",
      "                   ModelResponse(output=[ResponseOutputMessage(id='msg_0a967e5f7f7a333c00695d9943be14819490d780763286cd4a', content=[ResponseOutputText(annotations=[], text='Sun warms glass and stone,  \\nBlue sky folds the city bright—  \\nSunny streets hum life.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')],\n",
      "                                 usage=Usage(requests=1,\n",
      "                                             input_tokens=150,\n",
      "                                             input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
      "                                             output_tokens=24,\n",
      "                                             output_tokens_details=OutputTokensDetails(reasoning_tokens=0),\n",
      "                                             total_tokens=174,\n",
      "                                             request_usage_entries=[]),\n",
      "                                 response_id='resp_0a967e5f7f7a333c00695d994329b081949fa85409fe8f1a50')],\n",
      " 'tool_input_guardrail_results': [],\n",
      " 'tool_output_guardrail_results': []}\n"
     ]
    }
   ],
   "source": [
    "print(type(result))\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(result.__dict__, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbcf5d8",
   "metadata": {},
   "source": [
    "## Non Open-AI models\n",
    "\n",
    "Use Hugging Face models with OpenAI Agents SDK.\n",
    "\n",
    "Just add the HF_TOKEN env variable. And set the model param to:\n",
    "\n",
    "```\n",
    "litellm/huggingface/<provider>/<hf_org_or_user>/<hf_model>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e4f0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cef7e8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun shines in New York\n",
      "Golden light bathes the streets\n",
      "City walks in warmth\n"
     ]
    }
   ],
   "source": [
    "# using kimi-k2-thinking\n",
    "\n",
    "from agents import Agent, Runner, ModelSettings\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "\n",
    "model_minimax = LitellmModel(\n",
    "    model=\"huggingface/novita/MiniMaxAI/MiniMax-M2.1\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "agent_kimi = Agent(\n",
    "    name=\"Kimi agent\",\n",
    "    instructions=\"Always respond in haiku form\",\n",
    "    # model=\"litellm/huggingface/novita/MiniMaxAI/MiniMax-M2.1\",\n",
    "    tools=[get_weather],\n",
    "    model=model_minimax,\n",
    "    # optional, for tracing (requires openai API key)\n",
    "    model_settings=ModelSettings(include_usage=True,),\n",
    ")\n",
    "\n",
    "result = await Runner.run(agent_kimi, \"What's the weather in New York?\")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1cfd9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_last_agent': Agent(name='Kimi agent',\n",
      "                      handoff_description=None,\n",
      "                      tools=[FunctionTool(name='get_weather',\n",
      "                                          description='returns weather info '\n",
      "                                                      'for the specified city.',\n",
      "                                          params_json_schema={'additionalProperties': False,\n",
      "                                                              'properties': {'city': {'title': 'City',\n",
      "                                                                                      'type': 'string'}},\n",
      "                                                              'required': ['city'],\n",
      "                                                              'title': 'get_weather_args',\n",
      "                                                              'type': 'object'},\n",
      "                                          on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7b64259bf1a0>,\n",
      "                                          strict_json_schema=True,\n",
      "                                          is_enabled=True,\n",
      "                                          tool_input_guardrails=None,\n",
      "                                          tool_output_guardrails=None)],\n",
      "                      mcp_servers=[],\n",
      "                      mcp_config={},\n",
      "                      instructions='Always respond in haiku form',\n",
      "                      prompt=None,\n",
      "                      handoffs=[],\n",
      "                      model=<agents.extensions.models.litellm_model.LitellmModel object at 0x7b6418a6fe00>,\n",
      "                      model_settings=ModelSettings(temperature=None,\n",
      "                                                   top_p=None,\n",
      "                                                   frequency_penalty=None,\n",
      "                                                   presence_penalty=None,\n",
      "                                                   tool_choice=None,\n",
      "                                                   parallel_tool_calls=None,\n",
      "                                                   truncation=None,\n",
      "                                                   max_tokens=None,\n",
      "                                                   reasoning=None,\n",
      "                                                   verbosity=None,\n",
      "                                                   metadata=None,\n",
      "                                                   store=None,\n",
      "                                                   prompt_cache_retention=None,\n",
      "                                                   include_usage=True,\n",
      "                                                   response_include=None,\n",
      "                                                   top_logprobs=None,\n",
      "                                                   extra_query=None,\n",
      "                                                   extra_body=None,\n",
      "                                                   extra_headers=None,\n",
      "                                                   extra_args=None),\n",
      "                      input_guardrails=[],\n",
      "                      output_guardrails=[],\n",
      "                      output_type=None,\n",
      "                      hooks=None,\n",
      "                      tool_use_behavior='run_llm_again',\n",
      "                      reset_tool_choice=True),\n",
      " '_last_agent_ref': <weakref at 0x7b64186d5fd0; to 'Agent' at 0x7b64253a5df0>,\n",
      " 'context_wrapper': RunContextWrapper(context=None,\n",
      "                                      usage=Usage(requests=2,\n",
      "                                                  input_tokens=415,\n",
      "                                                  input_tokens_details=InputTokensDetails(cached_tokens=415),\n",
      "                                                  output_tokens=987,\n",
      "                                                  output_tokens_details=OutputTokensDetails(reasoning_tokens=0),\n",
      "                                                  total_tokens=1402,\n",
      "                                                  request_usage_entries=[RequestUsage(input_tokens=184,\n",
      "                                                                                      output_tokens=330,\n",
      "                                                                                      total_tokens=514,\n",
      "                                                                                      input_tokens_details=InputTokensDetails(cached_tokens=184),\n",
      "                                                                                      output_tokens_details=OutputTokensDetails(reasoning_tokens=0)),\n",
      "                                                                         RequestUsage(input_tokens=231,\n",
      "                                                                                      output_tokens=657,\n",
      "                                                                                      total_tokens=888,\n",
      "                                                                                      input_tokens_details=InputTokensDetails(cached_tokens=231),\n",
      "                                                                                      output_tokens_details=OutputTokensDetails(reasoning_tokens=0))])),\n",
      " 'final_output': 'Sunlight hits New York\\n'\n",
      "                 'Golden streams of light\\n'\n",
      "                 'City glows warmly',\n",
      " 'input': \"What's the weather in New York?\",\n",
      " 'input_guardrail_results': [],\n",
      " 'new_items': [ReasoningItem(agent=Agent(name='Kimi agent',\n",
      "                                         handoff_description=None,\n",
      "                                         tools=[FunctionTool(name='get_weather',\n",
      "                                                             description='returns '\n",
      "                                                                         'weather '\n",
      "                                                                         'info '\n",
      "                                                                         'for '\n",
      "                                                                         'the '\n",
      "                                                                         'specified '\n",
      "                                                                         'city.',\n",
      "                                                             params_json_schema={'additionalProperties': False,\n",
      "                                                                                 'properties': {'city': {'title': 'City',\n",
      "                                                                                                         'type': 'string'}},\n",
      "                                                                                 'required': ['city'],\n",
      "                                                                                 'title': 'get_weather_args',\n",
      "                                                                                 'type': 'object'},\n",
      "                                                             on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7b64259bf1a0>,\n",
      "                                                             strict_json_schema=True,\n",
      "                                                             is_enabled=True,\n",
      "                                                             tool_input_guardrails=None,\n",
      "                                                             tool_output_guardrails=None)],\n",
      "                                         mcp_servers=[],\n",
      "                                         mcp_config={},\n",
      "                                         instructions='Always respond in haiku '\n",
      "                                                      'form',\n",
      "                                         prompt=None,\n",
      "                                         handoffs=[],\n",
      "                                         model=<agents.extensions.models.litellm_model.LitellmModel object at 0x7b6418a6fe00>,\n",
      "                                         model_settings=ModelSettings(temperature=None,\n",
      "                                                                      top_p=None,\n",
      "                                                                      frequency_penalty=None,\n",
      "                                                                      presence_penalty=None,\n",
      "                                                                      tool_choice=None,\n",
      "                                                                      parallel_tool_calls=None,\n",
      "                                                                      truncation=None,\n",
      "                                                                      max_tokens=None,\n",
      "                                                                      reasoning=None,\n",
      "                                                                      verbosity=None,\n",
      "                                                                      metadata=None,\n",
      "                                                                      store=None,\n",
      "                                                                      prompt_cache_retention=None,\n",
      "                                                                      include_usage=True,\n",
      "                                                                      response_include=None,\n",
      "                                                                      top_logprobs=None,\n",
      "                                                                      extra_query=None,\n",
      "                                                                      extra_body=None,\n",
      "                                                                      extra_headers=None,\n",
      "                                                                      extra_args=None),\n",
      "                                         input_guardrails=[],\n",
      "                                         output_guardrails=[],\n",
      "                                         output_type=None,\n",
      "                                         hooks=None,\n",
      "                                         tool_use_behavior='run_llm_again',\n",
      "                                         reset_tool_choice=True),\n",
      "                             raw_item=ResponseReasoningItem(id='__fake_id__', summary=[Summary(text='Okay, let me think about this. The user is asking for the weather in New York. This is a straightforward request that I can fulfill using the tools I have available.\\n\\nI see that I have access to a \"get_weather\" tool that returns weather information for a specified city. Looking at the tool\\'s parameters, it requires a \"city\" parameter which should be a string. The user has clearly asked for the weather in \"New York,\" so I have all the information I need to make this tool call.\\n\\nLet me structure the tool call properly. I need to:\\n1. Use the \"get_weather\" tool\\n2. Pass \"New York\" as the city parameter\\n\\nThe format for the tool call should follow the specified structure with the tool_calls XML tags and the JSON object inside. The JSON object should have the name of the tool and the arguments in a \"properties\" object.\\n\\nSo I\\'ll create a tool call with:\\n- name: \"get_weather\"\\n- arguments: {\"city\": \"New York\"}\\n\\nThis should return the current weather information for New York, which is exactly what the user requested. The tool will handle the actual weather data retrieval, and then I can provide that information to the user in my response.\\n\\nI don\\'t need to add any additional explanation before making the tool call since the user\\'s request is clear and direct. After I get the weather data back from the tool, I can then provide a complete response about the current weather conditions in New York.', type='summary_text')], type='reasoning', content=None, encrypted_content=None, status=None, provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': '098b1343ef1228620247e7f3743c32b0'}),\n",
      "                             type='reasoning_item'),\n",
      "               ToolCallItem(agent=Agent(name='Kimi agent',\n",
      "                                        handoff_description=None,\n",
      "                                        tools=[FunctionTool(name='get_weather',\n",
      "                                                            description='returns '\n",
      "                                                                        'weather '\n",
      "                                                                        'info '\n",
      "                                                                        'for '\n",
      "                                                                        'the '\n",
      "                                                                        'specified '\n",
      "                                                                        'city.',\n",
      "                                                            params_json_schema={'additionalProperties': False,\n",
      "                                                                                'properties': {'city': {'title': 'City',\n",
      "                                                                                                        'type': 'string'}},\n",
      "                                                                                'required': ['city'],\n",
      "                                                                                'title': 'get_weather_args',\n",
      "                                                                                'type': 'object'},\n",
      "                                                            on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7b64259bf1a0>,\n",
      "                                                            strict_json_schema=True,\n",
      "                                                            is_enabled=True,\n",
      "                                                            tool_input_guardrails=None,\n",
      "                                                            tool_output_guardrails=None)],\n",
      "                                        mcp_servers=[],\n",
      "                                        mcp_config={},\n",
      "                                        instructions='Always respond in haiku '\n",
      "                                                     'form',\n",
      "                                        prompt=None,\n",
      "                                        handoffs=[],\n",
      "                                        model=<agents.extensions.models.litellm_model.LitellmModel object at 0x7b6418a6fe00>,\n",
      "                                        model_settings=ModelSettings(temperature=None,\n",
      "                                                                     top_p=None,\n",
      "                                                                     frequency_penalty=None,\n",
      "                                                                     presence_penalty=None,\n",
      "                                                                     tool_choice=None,\n",
      "                                                                     parallel_tool_calls=None,\n",
      "                                                                     truncation=None,\n",
      "                                                                     max_tokens=None,\n",
      "                                                                     reasoning=None,\n",
      "                                                                     verbosity=None,\n",
      "                                                                     metadata=None,\n",
      "                                                                     store=None,\n",
      "                                                                     prompt_cache_retention=None,\n",
      "                                                                     include_usage=True,\n",
      "                                                                     response_include=None,\n",
      "                                                                     top_logprobs=None,\n",
      "                                                                     extra_query=None,\n",
      "                                                                     extra_body=None,\n",
      "                                                                     extra_headers=None,\n",
      "                                                                     extra_args=None),\n",
      "                                        input_guardrails=[],\n",
      "                                        output_guardrails=[],\n",
      "                                        output_type=None,\n",
      "                                        hooks=None,\n",
      "                                        tool_use_behavior='run_llm_again',\n",
      "                                        reset_tool_choice=True),\n",
      "                            raw_item=ResponseFunctionToolCall(arguments='{\"city\":\"New York\"}', call_id='call_function_px6craxybtqi_1', name='get_weather', type='function_call', id='__fake_id__', status=None, provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': '098b1343ef1228620247e7f3743c32b0'}),\n",
      "                            type='tool_call_item'),\n",
      "               ToolCallOutputItem(agent=Agent(name='Kimi agent',\n",
      "                                              handoff_description=None,\n",
      "                                              tools=[FunctionTool(name='get_weather',\n",
      "                                                                  description='returns '\n",
      "                                                                              'weather '\n",
      "                                                                              'info '\n",
      "                                                                              'for '\n",
      "                                                                              'the '\n",
      "                                                                              'specified '\n",
      "                                                                              'city.',\n",
      "                                                                  params_json_schema={'additionalProperties': False,\n",
      "                                                                                      'properties': {'city': {'title': 'City',\n",
      "                                                                                                              'type': 'string'}},\n",
      "                                                                                      'required': ['city'],\n",
      "                                                                                      'title': 'get_weather_args',\n",
      "                                                                                      'type': 'object'},\n",
      "                                                                  on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7b64259bf1a0>,\n",
      "                                                                  strict_json_schema=True,\n",
      "                                                                  is_enabled=True,\n",
      "                                                                  tool_input_guardrails=None,\n",
      "                                                                  tool_output_guardrails=None)],\n",
      "                                              mcp_servers=[],\n",
      "                                              mcp_config={},\n",
      "                                              instructions='Always respond in '\n",
      "                                                           'haiku form',\n",
      "                                              prompt=None,\n",
      "                                              handoffs=[],\n",
      "                                              model=<agents.extensions.models.litellm_model.LitellmModel object at 0x7b6418a6fe00>,\n",
      "                                              model_settings=ModelSettings(temperature=None,\n",
      "                                                                           top_p=None,\n",
      "                                                                           frequency_penalty=None,\n",
      "                                                                           presence_penalty=None,\n",
      "                                                                           tool_choice=None,\n",
      "                                                                           parallel_tool_calls=None,\n",
      "                                                                           truncation=None,\n",
      "                                                                           max_tokens=None,\n",
      "                                                                           reasoning=None,\n",
      "                                                                           verbosity=None,\n",
      "                                                                           metadata=None,\n",
      "                                                                           store=None,\n",
      "                                                                           prompt_cache_retention=None,\n",
      "                                                                           include_usage=True,\n",
      "                                                                           response_include=None,\n",
      "                                                                           top_logprobs=None,\n",
      "                                                                           extra_query=None,\n",
      "                                                                           extra_body=None,\n",
      "                                                                           extra_headers=None,\n",
      "                                                                           extra_args=None),\n",
      "                                              input_guardrails=[],\n",
      "                                              output_guardrails=[],\n",
      "                                              output_type=None,\n",
      "                                              hooks=None,\n",
      "                                              tool_use_behavior='run_llm_again',\n",
      "                                              reset_tool_choice=True),\n",
      "                                  raw_item={'call_id': 'call_function_px6craxybtqi_1',\n",
      "                                            'output': 'The weather in New York '\n",
      "                                                      'is sunny',\n",
      "                                            'type': 'function_call_output'},\n",
      "                                  output='The weather in New York is sunny',\n",
      "                                  type='tool_call_output_item'),\n",
      "               ReasoningItem(agent=Agent(name='Kimi agent',\n",
      "                                         handoff_description=None,\n",
      "                                         tools=[FunctionTool(name='get_weather',\n",
      "                                                             description='returns '\n",
      "                                                                         'weather '\n",
      "                                                                         'info '\n",
      "                                                                         'for '\n",
      "                                                                         'the '\n",
      "                                                                         'specified '\n",
      "                                                                         'city.',\n",
      "                                                             params_json_schema={'additionalProperties': False,\n",
      "                                                                                 'properties': {'city': {'title': 'City',\n",
      "                                                                                                         'type': 'string'}},\n",
      "                                                                                 'required': ['city'],\n",
      "                                                                                 'title': 'get_weather_args',\n",
      "                                                                                 'type': 'object'},\n",
      "                                                             on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7b64259bf1a0>,\n",
      "                                                             strict_json_schema=True,\n",
      "                                                             is_enabled=True,\n",
      "                                                             tool_input_guardrails=None,\n",
      "                                                             tool_output_guardrails=None)],\n",
      "                                         mcp_servers=[],\n",
      "                                         mcp_config={},\n",
      "                                         instructions='Always respond in haiku '\n",
      "                                                      'form',\n",
      "                                         prompt=None,\n",
      "                                         handoffs=[],\n",
      "                                         model=<agents.extensions.models.litellm_model.LitellmModel object at 0x7b6418a6fe00>,\n",
      "                                         model_settings=ModelSettings(temperature=None,\n",
      "                                                                      top_p=None,\n",
      "                                                                      frequency_penalty=None,\n",
      "                                                                      presence_penalty=None,\n",
      "                                                                      tool_choice=None,\n",
      "                                                                      parallel_tool_calls=None,\n",
      "                                                                      truncation=None,\n",
      "                                                                      max_tokens=None,\n",
      "                                                                      reasoning=None,\n",
      "                                                                      verbosity=None,\n",
      "                                                                      metadata=None,\n",
      "                                                                      store=None,\n",
      "                                                                      prompt_cache_retention=None,\n",
      "                                                                      include_usage=True,\n",
      "                                                                      response_include=None,\n",
      "                                                                      top_logprobs=None,\n",
      "                                                                      extra_query=None,\n",
      "                                                                      extra_body=None,\n",
      "                                                                      extra_headers=None,\n",
      "                                                                      extra_args=None),\n",
      "                                         input_guardrails=[],\n",
      "                                         output_guardrails=[],\n",
      "                                         output_type=None,\n",
      "                                         hooks=None,\n",
      "                                         tool_use_behavior='run_llm_again',\n",
      "                                         reset_tool_choice=True),\n",
      "                             raw_item=ResponseReasoningItem(id='__fake_id__', summary=[Summary(text='Okay, the user asked about the weather in New York, so I called the get_weather tool to fetch that information. The tool returned that it\\'s sunny in New York.\\n\\nNow I need to respond to the user in the haiku form as instructed in my system prompt. A haiku is a traditional Japanese poem with a 5-7-5 syllable structure, usually about nature or seasons.\\n\\nLet me think of a weather-related haiku. Since the tool told me it\\'s sunny in New York, I should incorporate that into my response.\\n\\nLet me try:\\n\"Sunshine in the city\" (5 syllables: Sun-shine in the cit-y)\\n\"New York\\'s bright golden light\" (6 syllables - need to adjust: New York\\'s bright gold-en light)\\n\"Warmth spreads everywhere\" (5 syllables: Warmth spreads ev-ery-where)\\n\\nLet me count again:\\nLine 1: Sun-shine (2) in (1) the (1) cit-y (2) = 6 syllables... hmm, that\\'s off.\\n\\nLet me try again:\\n\"New York bathed in sun\" (5 syllables: New York bathed in sun)\\n\"Golden light on each street\" (6 syllables - need to adjust)\\n\"Warmth embraces all\" (5 syllables: Warmth em-braces all)\\n\\nStill not quite right. Let me try:\\n\"Sunlight bathes the city\" (6 syllables - need to adjust)\\n\"New York gleams in warm light\" (5 syllables: New York gleams in warm light)\\n\"Everyone smiles bright\" (5 syllables: Ev-ery-one smiles bright)\\n\\nHmm, I\\'m struggling a bit with the syllable count. Let me try simpler lines:\\n\"Sunny New York day\" (5 syllables: Sun-ny New York day)\\n\"Golden light all around\" (6 syllables - need to adjust)\\n\"Joy in every step\" (6 syllables - need to adjust)\\n\\nLet me try:\\n\"Sunny New York day\" (5 syllables: Sun-ny New York day)\\n\"Light cascading down\" (5 syllables: Light cas-cad-ing down)\\n\"Everyone smiles\" (4 syllables - need to adjust)\\n\\nLet me try one more time:\\n\"Sun shines on NYC\" (5 syllables: Sun shines on N-Y-C)\\n\"People walk in warmth\" (5 syllables: Peo-ple walk in warmth)\\n\"Spring has finally come\" (6 syllables - need to adjust)\\n\\nI think I need to simplify more:\\n\"Sunlight in New York\" (5 syllables: Sun-light in New York)\\n\"Golden rays shine down\" (6 syllables - need to adjust)\\n\"City bathed in gold\" (5 syllables: Cit-y bathed in gold)\\n\\nWait, I think I have it:\\n\"Sunlight hits New York\" (5 syllables: Sun-light hits New York)\\n\"Golden streams of light\" (5 syllables: Gold-en streams of light)\\n\"City glows warmly\" (5 syllables: Cit-y glows warm-ly)\\n\\nThat\\'s 5-5-5. Perfect! And it describes the sunny weather in New York that the tool reported. I\\'ll go with this response.', type='summary_text')], type='reasoning', content=None, encrypted_content=None, status=None, provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': 'ba2989bfa676d722579d2eecbf7a5b4e'}),\n",
      "                             type='reasoning_item'),\n",
      "               MessageOutputItem(agent=Agent(name='Kimi agent',\n",
      "                                             handoff_description=None,\n",
      "                                             tools=[FunctionTool(name='get_weather',\n",
      "                                                                 description='returns '\n",
      "                                                                             'weather '\n",
      "                                                                             'info '\n",
      "                                                                             'for '\n",
      "                                                                             'the '\n",
      "                                                                             'specified '\n",
      "                                                                             'city.',\n",
      "                                                                 params_json_schema={'additionalProperties': False,\n",
      "                                                                                     'properties': {'city': {'title': 'City',\n",
      "                                                                                                             'type': 'string'}},\n",
      "                                                                                     'required': ['city'],\n",
      "                                                                                     'title': 'get_weather_args',\n",
      "                                                                                     'type': 'object'},\n",
      "                                                                 on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7b64259bf1a0>,\n",
      "                                                                 strict_json_schema=True,\n",
      "                                                                 is_enabled=True,\n",
      "                                                                 tool_input_guardrails=None,\n",
      "                                                                 tool_output_guardrails=None)],\n",
      "                                             mcp_servers=[],\n",
      "                                             mcp_config={},\n",
      "                                             instructions='Always respond in '\n",
      "                                                          'haiku form',\n",
      "                                             prompt=None,\n",
      "                                             handoffs=[],\n",
      "                                             model=<agents.extensions.models.litellm_model.LitellmModel object at 0x7b6418a6fe00>,\n",
      "                                             model_settings=ModelSettings(temperature=None,\n",
      "                                                                          top_p=None,\n",
      "                                                                          frequency_penalty=None,\n",
      "                                                                          presence_penalty=None,\n",
      "                                                                          tool_choice=None,\n",
      "                                                                          parallel_tool_calls=None,\n",
      "                                                                          truncation=None,\n",
      "                                                                          max_tokens=None,\n",
      "                                                                          reasoning=None,\n",
      "                                                                          verbosity=None,\n",
      "                                                                          metadata=None,\n",
      "                                                                          store=None,\n",
      "                                                                          prompt_cache_retention=None,\n",
      "                                                                          include_usage=True,\n",
      "                                                                          response_include=None,\n",
      "                                                                          top_logprobs=None,\n",
      "                                                                          extra_query=None,\n",
      "                                                                          extra_body=None,\n",
      "                                                                          extra_headers=None,\n",
      "                                                                          extra_args=None),\n",
      "                                             input_guardrails=[],\n",
      "                                             output_guardrails=[],\n",
      "                                             output_type=None,\n",
      "                                             hooks=None,\n",
      "                                             tool_use_behavior='run_llm_again',\n",
      "                                             reset_tool_choice=True),\n",
      "                                 raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Sunlight hits New York\\nGolden streams of light\\nCity glows warmly', type='output_text', logprobs=[])], role='assistant', status='completed', type='message', provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': 'ba2989bfa676d722579d2eecbf7a5b4e'}),\n",
      "                                 type='message_output_item')],\n",
      " 'output_guardrail_results': [],\n",
      " 'raw_responses': [ModelResponse(output=[ResponseReasoningItem(id='__fake_id__', summary=[Summary(text='Okay, let me think about this. The user is asking for the weather in New York. This is a straightforward request that I can fulfill using the tools I have available.\\n\\nI see that I have access to a \"get_weather\" tool that returns weather information for a specified city. Looking at the tool\\'s parameters, it requires a \"city\" parameter which should be a string. The user has clearly asked for the weather in \"New York,\" so I have all the information I need to make this tool call.\\n\\nLet me structure the tool call properly. I need to:\\n1. Use the \"get_weather\" tool\\n2. Pass \"New York\" as the city parameter\\n\\nThe format for the tool call should follow the specified structure with the tool_calls XML tags and the JSON object inside. The JSON object should have the name of the tool and the arguments in a \"properties\" object.\\n\\nSo I\\'ll create a tool call with:\\n- name: \"get_weather\"\\n- arguments: {\"city\": \"New York\"}\\n\\nThis should return the current weather information for New York, which is exactly what the user requested. The tool will handle the actual weather data retrieval, and then I can provide that information to the user in my response.\\n\\nI don\\'t need to add any additional explanation before making the tool call since the user\\'s request is clear and direct. After I get the weather data back from the tool, I can then provide a complete response about the current weather conditions in New York.', type='summary_text')], type='reasoning', content=None, encrypted_content=None, status=None, provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': '098b1343ef1228620247e7f3743c32b0'}),\n",
      "                                         ResponseFunctionToolCall(arguments='{\"city\":\"New York\"}', call_id='call_function_px6craxybtqi_1', name='get_weather', type='function_call', id='__fake_id__', status=None, provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': '098b1343ef1228620247e7f3743c32b0'})],\n",
      "                                 usage=Usage(requests=1,\n",
      "                                             input_tokens=184,\n",
      "                                             input_tokens_details=InputTokensDetails(cached_tokens=184),\n",
      "                                             output_tokens=330,\n",
      "                                             output_tokens_details=OutputTokensDetails(reasoning_tokens=0),\n",
      "                                             total_tokens=514,\n",
      "                                             request_usage_entries=[]),\n",
      "                                 response_id=None),\n",
      "                   ModelResponse(output=[ResponseReasoningItem(id='__fake_id__', summary=[Summary(text='Okay, the user asked about the weather in New York, so I called the get_weather tool to fetch that information. The tool returned that it\\'s sunny in New York.\\n\\nNow I need to respond to the user in the haiku form as instructed in my system prompt. A haiku is a traditional Japanese poem with a 5-7-5 syllable structure, usually about nature or seasons.\\n\\nLet me think of a weather-related haiku. Since the tool told me it\\'s sunny in New York, I should incorporate that into my response.\\n\\nLet me try:\\n\"Sunshine in the city\" (5 syllables: Sun-shine in the cit-y)\\n\"New York\\'s bright golden light\" (6 syllables - need to adjust: New York\\'s bright gold-en light)\\n\"Warmth spreads everywhere\" (5 syllables: Warmth spreads ev-ery-where)\\n\\nLet me count again:\\nLine 1: Sun-shine (2) in (1) the (1) cit-y (2) = 6 syllables... hmm, that\\'s off.\\n\\nLet me try again:\\n\"New York bathed in sun\" (5 syllables: New York bathed in sun)\\n\"Golden light on each street\" (6 syllables - need to adjust)\\n\"Warmth embraces all\" (5 syllables: Warmth em-braces all)\\n\\nStill not quite right. Let me try:\\n\"Sunlight bathes the city\" (6 syllables - need to adjust)\\n\"New York gleams in warm light\" (5 syllables: New York gleams in warm light)\\n\"Everyone smiles bright\" (5 syllables: Ev-ery-one smiles bright)\\n\\nHmm, I\\'m struggling a bit with the syllable count. Let me try simpler lines:\\n\"Sunny New York day\" (5 syllables: Sun-ny New York day)\\n\"Golden light all around\" (6 syllables - need to adjust)\\n\"Joy in every step\" (6 syllables - need to adjust)\\n\\nLet me try:\\n\"Sunny New York day\" (5 syllables: Sun-ny New York day)\\n\"Light cascading down\" (5 syllables: Light cas-cad-ing down)\\n\"Everyone smiles\" (4 syllables - need to adjust)\\n\\nLet me try one more time:\\n\"Sun shines on NYC\" (5 syllables: Sun shines on N-Y-C)\\n\"People walk in warmth\" (5 syllables: Peo-ple walk in warmth)\\n\"Spring has finally come\" (6 syllables - need to adjust)\\n\\nI think I need to simplify more:\\n\"Sunlight in New York\" (5 syllables: Sun-light in New York)\\n\"Golden rays shine down\" (6 syllables - need to adjust)\\n\"City bathed in gold\" (5 syllables: Cit-y bathed in gold)\\n\\nWait, I think I have it:\\n\"Sunlight hits New York\" (5 syllables: Sun-light hits New York)\\n\"Golden streams of light\" (5 syllables: Gold-en streams of light)\\n\"City glows warmly\" (5 syllables: Cit-y glows warm-ly)\\n\\nThat\\'s 5-5-5. Perfect! And it describes the sunny weather in New York that the tool reported. I\\'ll go with this response.', type='summary_text')], type='reasoning', content=None, encrypted_content=None, status=None, provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': 'ba2989bfa676d722579d2eecbf7a5b4e'}),\n",
      "                                         ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Sunlight hits New York\\nGolden streams of light\\nCity glows warmly', type='output_text', logprobs=[])], role='assistant', status='completed', type='message', provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': 'ba2989bfa676d722579d2eecbf7a5b4e'})],\n",
      "                                 usage=Usage(requests=1,\n",
      "                                             input_tokens=231,\n",
      "                                             input_tokens_details=InputTokensDetails(cached_tokens=231),\n",
      "                                             output_tokens=657,\n",
      "                                             output_tokens_details=OutputTokensDetails(reasoning_tokens=0),\n",
      "                                             total_tokens=888,\n",
      "                                             request_usage_entries=[]),\n",
      "                                 response_id=None)],\n",
      " 'tool_input_guardrail_results': [],\n",
      " 'tool_output_guardrail_results': []}\n"
     ]
    }
   ],
   "source": [
    "pprint(result.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8393c03",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebccb06d",
   "metadata": {},
   "source": [
    "### Structured Output\n",
    "\n",
    "- Use the `output_type` param with Pydantic objects (or dataclasses, lists, TypedDicts, etc.)\n",
    "- When using non-openai models, remember to check that they support both Structured Ouput AND tool calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f84cc7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Meeting with Alice and Bob' date='July 5th' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from agents import Agent\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "model = LitellmModel(\n",
    "    model=\"huggingface/novita/zai-org/GLM-4.7\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Calendar extractor\",\n",
    "    instructions=\"Extract calendar events from text\",\n",
    "    output_type=CalendarEvent,\n",
    "    model=model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "result = await Runner.run(\n",
    "    agent,\n",
    "    \"Extract the event from the following text: 'Meeting with Alice and Bob on July 5th.'\",\n",
    ")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846818ce",
   "metadata": {},
   "source": [
    "### Multi-Agent systems\n",
    "\n",
    "Two main architectures:\n",
    "- Manager (agents as tools): A central manager/orchestrator invokes specialized sub‑agents as tools and retains control of the conversation.\n",
    "- Handoffs: Peer agents hand off control to a specialized agent that takes over the conversation. This is decentralized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0549c",
   "metadata": {},
   "source": [
    "#### Agents Handoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4055da02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The causes of World War II are complex and rooted deeply in the aftermath of World War I. While the immediate trigger was the invasion of Poland, the conflict was the result of unresolved political issues, economic instability, and the rise of aggressive totalitarian regimes during the 1930s.\n",
      "\n",
      "Here is a breakdown of the primary causes and context:\n",
      "\n",
      "### 1. The Aftermath of World War I\n",
      "The peace settlement that ended WWI, particularly the **Treaty of Versailles (1919)**, left a lasting resentment in Germany.\n",
      "*   **War Guilt Clause:** Germany was forced to accept full responsibility for the war, which was a humiliation for the German people.\n",
      "*   **Reparations:** Germany was hit with massive financial reparations it couldn't afford, leading to hyperinflation and economic collapse in the early 1920s.\n",
      "*   **Territorial Losses:** Germany lost significant territory (such as Alsace-Lorraine to France and the Polish Corridor) and was stripped of its overseas colonies and military power.\n",
      "\n",
      "### 2. Global Economic Instability\n",
      "The **Great Depression (1929)** destabilized economies worldwide.\n",
      "*   In Germany, high unemployment and poverty created a desperate environment ripe for radical political solutions.\n",
      "*   This economic turmoil allowed Adolf Hitler and the **Nazi Party** to rise to power in 1933. Hitler promised to tear up the Treaty of Versailles, restore the German military, and provide for the people.\n",
      "\n",
      "### 3. Rise of Totalitarianism and Expansionism\n",
      "Three major nations—Germany, Italy, and Japan—fell under the control of aggressive dictatorships with expansionist goals.\n",
      "*   **Germany (Nazism):** Hitler sought to unite all German-speaking people (*Lebensraum* or \"living space\") and dominate Europe.\n",
      "*   **Italy (Fascism):** Benito Mussolini aimed to restore the Roman Empire by invading North Africa and the Balkans.\n",
      "*   **Japan (Militarism):** Japan, lacking natural resources, sought to dominate Asia and the Pacific to secure oil and rubber. They invaded Manchuria in 1931 and China in 1937.\n",
      "\n",
      "### 4. Failure of the League of Nations\n",
      "Established after WWI to prevent future conflicts, the **League of Nations** proved ineffective.\n",
      "*   It had no military power to enforce its decisions.\n",
      "*   The United States never joined, and major powers were often distracted by their own domestic issues.\n",
      "*   When Japan invaded Manchuria and Italy invaded Ethiopia in the 1930s, the League imposed weak sanctions that did nothing to stop the aggression. This failure signaled to Hitler that acts of aggression would go unpunished.\n",
      "\n",
      "### 5. The Policy of Appeasement\n",
      "Rather than confronting Hitler immediately, Britain and France adopted a policy known as **appeasement**. Having been traumatized by WWI, they were desperate to avoid another war.\n",
      "*   They allowed Germany to violate the Treaty of Versailles, such as rebuilding its army and reoccupying the Rhineland.\n",
      "*   The critical moment came in 1938 with the **Munich Agreement**, where Britain and France gave Hitler the **Sudetenland** (part of Czechoslovakia) in exchange for a promise of peace. Hitler broke this promise almost immediately by taking the rest of Czechoslovakia.\n",
      "\n",
      "### 6. The Nazi-Soviet Pact\n",
      "In August 1939, just days before the war began, Germany and the Soviet Union signed a **Non-Aggression Pact**.\n",
      "*   This was a shock to the world because the two regimes were ideological enemies.\n",
      "*   Secretly, they agreed to divide Poland between them.\n",
      "*   This pact meant Hitler could invade Poland without facing a two-front war (fighting the Soviets in the East and the Allies in the West simultaneously).\n",
      "\n",
      "### The Immediate Trigger\n",
      "On **September 1, 1939**, German forces invaded Poland using **Blitzkrieg** tactics (\"lightning war\"). Two days later, on September 3, Britain and France honored their guarantees to protect Poland and declared war on Germany, officially beginning World War II in Europe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent\n",
    "\n",
    "glm_model = LitellmModel(\n",
    "    model=\"huggingface/novita/zai-org/GLM-4.7\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "history_tutor_agent = Agent(\n",
    "    name=\"History Tutor\",\n",
    "    handoff_description=\"Specialist agent for historical questions\",\n",
    "    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "math_tutor_agent = Agent(\n",
    "    name=\"Math Tutor\",\n",
    "    handoff_description=\"Specialist agent for math questions\",\n",
    "    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=\"You determine which agent to use based on the user's homework question\",\n",
    "    handoffs=[history_tutor_agent, math_tutor_agent],\n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "result = await Runner.run(\n",
    "    triage_agent,\n",
    "    \"Can you explain the causes of World War II?\",\n",
    ")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebdb44",
   "metadata": {},
   "source": [
    "#### Agents As Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d2e5b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The causes of World War II were complex and rooted in unresolved issues from World War I, economic instability, and the rise of aggressive totalitarian regimes. Here's a breakdown:\n",
      "\n",
      "## 1. The Treaty of Versailles (1919)\n",
      "- Imposed harsh penalties on Germany: accepted full responsibility, paid massive reparations, lost territory, faced military restrictions\n",
      "- Created deep resentment and humiliation, fueling radical political movements\n",
      "\n",
      "## 2. Rise of Totalitarianism and Fascism\n",
      "- **Adolf Hitler (Germany):** Promoted German superiority (*Lebensraum*) and sought to overturn the Treaty\n",
      "- **Benito Mussolini (Italy):** Pursued imperial ambitions in Africa and the Mediterranean\n",
      "- **Military Japan:** Sought domination of East Asia to secure resources\n",
      "\n",
      "## 3. Failure of the League of Nations\n",
      "- Lacked enforcement power and key member support\n",
      "- Weak response to Japan's invasion of Manchuria (1931) and Italy's invasion of Ethiopia (1935)\n",
      "- Emboldened aggressors by showing they could act without consequences\n",
      "\n",
      "## 4. The Great Depression (1929)\n",
      "- Global economic collapse destabilized democracies\n",
      "- Enabled radical parties to gain popularity by promising solutions\n",
      "\n",
      "## 5. Expansionism and Appeasement\n",
      "- Hitler violated the Treaty step by step (Rhineland 1936, Austria 1938, Sudetenland 1938)\n",
      "- Britain and France pursued **appeasement**—giving in to Hitler's demands to avoid war\n",
      "- The Munich Agreement (1938) allowed Germany to take the Sudetenland; Hitler's promise of peace was a lie\n",
      "\n",
      "## 6. The Nazi-Soviet Pact (August 1939)\n",
      "- Germany and the USSR signed a non-aggression treaty\n",
      "- Secretly agreed to divide Poland between them\n",
      "- Freed Hitler to invade Poland without fear of a two-front war\n",
      "\n",
      "## The Immediate Trigger: Invasion of Poland\n",
      "On **September 1, 1939**, Germany invaded Poland using Blitzkrieg tactics. Britain and France declared war on September 3, 1939, marking the official start of World War II in Europe.\n",
      "\n",
      "---\n",
      "\n",
      "In summary, while the invasion of Poland was the spark that ignited the war, deeper causes included resentment over WWI, economic desperation, international powerlessness, and unchecked fascist aggression.\n"
     ]
    }
   ],
   "source": [
    "manager_agent = Agent(\n",
    "    name=\"Manager Agent\",\n",
    "    instructions=\"You manage a team of agents to answer user questions effectively.\",\n",
    "    tools=[\n",
    "        history_tutor_agent.as_tool(\n",
    "            tool_name=\"history_tutor_agent\",\n",
    "            tool_description=\"Handles historical queries\",\n",
    "        ),\n",
    "        math_tutor_agent.as_tool(\n",
    "            tool_name=\"math_tutor_agent\",\n",
    "            tool_description=\"Handles math questions\",\n",
    "        ),\n",
    "    ],\n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "result = await Runner.run(\n",
    "    manager_agent,\n",
    "    \"Can you explain the causes of World War II?\",\n",
    ")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb128e37",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e2426",
   "metadata": {},
   "source": [
    "### Pre-built Tools\n",
    "\n",
    "OpenAI offers a few built-in tools when using the OpenAIResponsesModel:\n",
    "\n",
    "- The `WebSearchTool` lets an agent search the web.\n",
    "- The `FileSearchTool` allows retrieving information from your OpenAI Vector Stores.\n",
    "- The `ComputerTool` allows automating computer use tasks.\n",
    "- The `CodeInterpreterTool` lets the LLM execute code in a sandboxed environment.\n",
    "- The `HostedMCPTool` exposes a remote MCP server's tools to the model.\n",
    "- The `ImageGenerationTool` generates images from a prompt.\n",
    "- The `LocalShellTool` runs shell commands on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55c27453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of January 6, 2026 the President of the United States is Donald J. Trump (47th President), inaugurated January 20, 2025. ([en.wikipedia.org](https://en.wikipedia.org/wiki/Second_presidency_of_Donald_Trump?utm_source=openai))\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, WebSearchTool\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    tools=[\n",
    "        WebSearchTool(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(agent, \"Who is the current president of the United States as of 2026?\")\n",
    "    print(result.final_output)\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1353ac43",
   "metadata": {},
   "source": [
    "### Custom Tools\n",
    "\n",
    "You can define custom tools that the agent can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21850942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in San Francisco is sunny!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "ERROR:openai.agents:[non-fatal] Tracing: max retries reached, giving up on this batch.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contents of the file `/tmp/hello.txt` are:\n",
      "\n",
      "```\n",
      "Hello, World!\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "ERROR:openai.agents:[non-fatal] Tracing: max retries reached, giving up on this batch.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "ERROR:openai.agents:[non-fatal] Tracing: max retries reached, giving up on this batch.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing_extensions import TypedDict, Any\n",
    "from agents import Agent, FunctionTool, RunContextWrapper, function_tool\n",
    "\n",
    "\n",
    "class Location(TypedDict):\n",
    "    lat: float\n",
    "    long: float\n",
    "\n",
    "@function_tool  \n",
    "async def fetch_weather(location: Location) -> str:\n",
    "    \n",
    "    \"\"\"Fetch the weather for a given location.\n",
    "\n",
    "    Args:\n",
    "        location: The location to fetch the weather for.\n",
    "    \"\"\"\n",
    "    # In real life, we'd fetch the weather from a weather API\n",
    "    return f\"The weather in {location['lat']}, {location['long']} is sunny\"\n",
    "\n",
    "\n",
    "@function_tool(name_override=\"fetch_data\")  \n",
    "def read_file(ctx: RunContextWrapper[Any], path: str, directory: str | None = None) -> str:\n",
    "    \"\"\"Read the contents of a file.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the file to read.\n",
    "        directory: The directory to read the file from.\n",
    "    \"\"\"\n",
    "    # In real life, we'd read the file from the file system\n",
    "    return \"Hello, World!\"\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    tools=[fetch_weather, read_file],  \n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "result = await Runner.run(agent, \"What is the weather in San Francisco?\")\n",
    "print(result.final_output)\n",
    "\n",
    "result = await Runner.run(agent, \"What are the contents of the file /tmp/hello.txt?\")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21e976e",
   "metadata": {},
   "source": [
    "## Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "172ac28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco.\n",
      "California.\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, SQLiteSession\n",
    "\n",
    "# Create agent\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"Reply very concisely.\",\n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True)\n",
    ")\n",
    "\n",
    "# Create a new conversation\n",
    "session = SQLiteSession(session_id=\"conv_123\")\n",
    "\n",
    "# Optionally resume a previous conversation by passing a conversation ID\n",
    "# session = OpenAIConversationsSession(conversation_id=\"conv_123\")\n",
    "\n",
    "# Start conversation\n",
    "result = await Runner.run(\n",
    "    agent,\n",
    "    \"What city is the Golden Gate Bridge in?\",\n",
    "    session=session\n",
    ")\n",
    "print(result.final_output)  # \"San Francisco\"\n",
    "\n",
    "# Continue the conversation\n",
    "result = await Runner.run(\n",
    "    agent,\n",
    "    \"What state is it in?\",\n",
    "    session=session\n",
    ")\n",
    "print(result.final_output)  # \"California\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd09286",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04915265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 jokes for you:\n",
      "\n",
      "1.  **Why don't scientists trust atoms?**\n",
      "    Because they make up everything.\n",
      "\n",
      "2.  **Why did the scarecrow win an award?**\n",
      "    Because he was outstanding in his field.\n",
      "\n",
      "3.  **I told my wife she was drawing her eyebrows too high.**\n",
      "    She looked surprised.\n",
      "\n",
      "4.  **What do you call a fake noodle?**\n",
      "    An impasta.\n",
      "\n",
      "5.  **Parallel lines have so much in common.**\n",
      "    It’s a shame they’ll never meet."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "ERROR:openai.agents:[non-fatal] Tracing: max retries reached, giving up on this batch.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "ERROR:openai.agents:[non-fatal] Tracing: max retries reached, giving up on this batch.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "from agents import Agent, Runner\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"You are a helpful assistant.\",\n",
    "        model=glm_model,\n",
    "        model_settings=ModelSettings(include_usage=True)\n",
    "    )\n",
    "\n",
    "    result = Runner.run_streamed(agent, input=\"Please tell me 5 jokes.\")\n",
    "    async for event in result.stream_events():\n",
    "        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f63877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
